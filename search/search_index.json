{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"architecture/","title":"Architecture","text":"<p>How giddyanne works internally. Read this before changing the indexing pipeline, search logic, or server lifecycle.</p> <p>For user-facing docs, see the home page. For dev setup, see Contributing.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     HTTP      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Go CLI     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  FastAPI Server (http_main.py)   \u2502\n\u2502  (giddy)    \u2502               \u2502                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n                              \u2502  \u2502 engine \u2502 \u2502chunker \u2502           \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  stdio        \u2502  \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2502 Claude Code \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502      \u2502                           \u2502\n\u2502  (MCP)      \u2502               \u2502  \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502  \u2502 LanceDB        \u2502              \u2502\n                              \u2502  \u2502 (vectorstore)  \u2502              \u2502\n                              \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n                              \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n                              \u2502  \u2502 watchdog       \u2502              \u2502\n                              \u2502  \u2502 (file watcher) \u2502              \u2502\n                              \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                          \u2502 Unix socket\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502  Embed Server (embed_server.py)  \u2502\n                              \u2502  Global singleton, one per machine\u2502\n                              \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n                              \u2502  \u2502 sentence-transformers     \u2502    \u2502\n                              \u2502  \u2502 (loads model once ~727MB) \u2502    \u2502\n                              \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Three entry points into the Python core:</p> <ul> <li><code>http_main.py</code> starts an HTTP server per project. The Go CLI talks to it over HTTP.</li> <li><code>mcp_main.py</code> starts an MCP server over stdio. Claude Code talks to it directly.</li> <li><code>embed_server.py</code> is a global singleton that loads the embedding model once and serves all project servers over a Unix socket (<code>~/.local/state/giddyanne/embed.sock</code>). Auto-started by project servers, or managed directly via <code>giddy embed start|stop|status</code>.</li> </ul> <p>Project servers use <code>SharedEmbedding</code> to proxy embed calls to the shared server instead of loading the ~900MB model in-process. This means running 3 projects costs ~727MB (one model) + 3\u00d7~180MB (project servers) instead of 3\u00d7~900MB. Falls back to in-process <code>LocalEmbedding</code> if the shared server is unavailable.</p> <p>Both <code>http_main.py</code> and <code>mcp_main.py</code> call <code>src/startup.py</code> to initialize shared components (config, embeddings, vectorstore, indexer), then add their transport layer on top.</p>"},{"location":"architecture/#source-map","title":"Source Map","text":"<p>All Python source lives in <code>src/</code>. Each file has one job:</p> File What it does <code>engine.py</code> Orchestrates indexing \u2014 file discovery, mtime checks, batch processing. Start here. <code>chunker.py</code> Splits files into chunks using language-aware separators <code>embeddings.py</code> Embedding providers (local sentence-transformers, shared server, Ollama) <code>embed_lifecycle.py</code> Start/stop/health-check the shared embedding server <code>global_config.py</code> Global config loader for <code>~/.config/giddyanne/config.yaml</code> <code>vectorstore.py</code> All LanceDB operations \u2014 upsert, delete, search, caching <code>watcher.py</code> Watches the filesystem for changes, debounces events <code>api.py</code> FastAPI endpoints (<code>/search</code>, <code>/status</code>, <code>/stats</code>, <code>/health</code>) <code>mcp_server.py</code> MCP tool definitions for Claude Code <code>startup.py</code> Shared initialization \u2014 creates components, runs indexing, signal handlers <code>project_config.py</code> Loads <code>.giddyanne.yaml</code>, applies file filters, handles git-only mode <code>languages.py</code> Language definitions \u2014 extensions and chunk separators for 24 languages <code>errors.py</code> Custom exceptions <p>The Go CLI is a single file: <code>cmd/giddy/main.go</code>.</p>"},{"location":"architecture/#the-indexing-pipeline","title":"The Indexing Pipeline","text":"<p>This is the core of the system. It runs on startup and again whenever files change.</p>"},{"location":"architecture/#1-file-discovery","title":"1. File Discovery","text":"<p>The engine walks configured paths using <code>rglob(\"*\")</code>. Each candidate goes through <code>FileFilter.should_include()</code> in <code>project_config.py</code>:</p> <ul> <li>Does it have a supported extension? (defined in <code>languages.py</code>)</li> <li>Is it under the max file size? (default: 1MB)</li> <li>Does it match a <code>.gitignore</code> pattern? (auto-loaded)</li> <li>Does it match any configured <code>ignore_patterns</code>?</li> </ul> <p>Files that pass all checks become indexing candidates.</p>"},{"location":"architecture/#2-mtime-comparison","title":"2. Mtime Comparison","text":"<p>Before reading or embedding anything, the engine does a single bulk query to load all stored mtimes from the database (<code>get_all_mtimes()</code>). Each file's current mtime is compared against what's stored. Unchanged files are skipped entirely.</p> <p>This is the main performance optimization. After the first index, subsequent startups only process files that actually changed.</p>"},{"location":"architecture/#3-chunking","title":"3. Chunking","text":"<p>Files are split into pieces in <code>chunker.py</code>. The chunker uses tree-sitter AST parsing to split code at real node boundaries \u2014 functions, classes, and other top-level definitions. Falls back to blank-line splitting for languages without a tree-sitter grammar.</p> <p>After splitting: 1. Small chunks get merged \u2014 consecutive chunks under <code>min_lines</code> (default: 10) are combined 2. Large chunks get split \u2014 chunks over <code>max_lines</code> (default: 50) are broken up with <code>overlap_lines</code> (default: 5) lines of overlap 3. Token-aware splitting \u2014 after line-based chunking, oversized chunks are recursively halved until they fit the model's token budget (256 tokens for MiniLM). This eliminates silent embedding truncation. 4. Context enrichment \u2014 each chunk gets file path and function/class name prepended before embedding. The stored content stays raw (search results show code only).</p>"},{"location":"architecture/#4-embedding","title":"4. Embedding","text":"<p>Each chunk gets three embeddings (384-dimensional vectors from <code>all-MiniLM-L6-v2</code>):</p> <ul> <li>path_embedding \u2014 the file path. Helps queries like \"tests for auth\" find test files.</li> <li>content_embedding \u2014 the actual code. This is the primary search signal.</li> <li>description_embedding \u2014 from the config's path description (e.g., \"Authentication and session management\"). Gives the model extra context about what this code is for.</li> </ul> <p>Chunks are batched (32 per batch) and embedded in a single model call. File reads are parallelized with an asyncio semaphore (16 concurrent). The goal is to minimize the number of model invocations \u2014 that's where the time goes.</p>"},{"location":"architecture/#5-storage","title":"5. Storage","text":"<p>Everything is upserted into LanceDB. The schema per row:</p> <pre><code>path, chunk_index, start_line, end_line, content, fts_content, description, mtime,\npath_embedding[384], content_embedding[384], description_embedding[384]\n</code></pre> <p>Each file produces multiple rows (one per chunk). When a file is re-indexed, all its existing rows are deleted first, then the new chunks are inserted.</p>"},{"location":"architecture/#how-search-works","title":"How Search Works","text":"<p>Search lives in <code>vectorstore.py</code>. There are three modes:</p> <p>Semantic \u2014 embeds the query, does vector similarity search against content embeddings, and blends in description embedding scores. The blend is <code>(1 - desc_weight) * content_score + desc_weight * desc_score</code> with a default description weight of 0.3.</p> <p>Full-text \u2014 BM25 keyword search via LanceDB's built-in FTS index. The index uses an enriched <code>fts_content</code> column that repeats file path and symbol name 3x before raw content, simulating BM25F field-level boosting. Pure keyword matching with stemming and stop word removal.</p> <p>Hybrid (default) \u2014 runs both, then combines results using Reciprocal Rank Fusion (RRF) with tunable weights (semantic 1.2, FTS 0.5). Results are biased by file category (source 1.0x, tests 0.8x, docs 0.6x) and deduplicated by file (best chunk per file). Falls back to semantic-only if the FTS index isn't available yet.</p>"},{"location":"architecture/#query-caching","title":"Query Caching","text":"<p>Query embeddings are cached in a separate LanceDB table. If you search for \"auth logic\" twice, the second time skips the embedding step entirely. The cache tracks hit counts and last-used timestamps.</p>"},{"location":"architecture/#file-watching","title":"File Watching","text":"<p>The watcher (<code>watcher.py</code>) uses the <code>watchdog</code> library to monitor configured paths for filesystem events.</p> <p>Key behaviors: - Debouncing \u2014 events are coalesced over a 100ms window. If you save a file three times in quick succession, it only re-indexes once. - Event precedence \u2014 DELETE &gt; CREATED &gt; MODIFIED. If a file is created and then deleted within the debounce window, only the delete fires. - Filtering \u2014 events are checked against <code>FileFilter.matches_path()</code> before processing. Changes to ignored files are silently dropped. - Async bridge \u2014 watchdog is synchronous, so the watcher bridges events into the async event loop for the indexer callback.</p> <p>When a file change fires: 1. Delete all existing chunks for that path 2. Re-read the file 3. Re-chunk with language-aware splitting 4. Re-embed all chunks 5. Upsert to the database</p> <p>For deletes, it just removes all chunks for that path.</p>"},{"location":"architecture/#startup-sequence","title":"Startup Sequence","text":""},{"location":"architecture/#http-server-http_mainpy","title":"HTTP Server (<code>http_main.py</code>)","text":"<ol> <li>Parse CLI args (<code>--daemon</code>, <code>--port</code>, <code>--host</code>, <code>--verbose</code>, <code>--path</code>)</li> <li>Load config from <code>.giddyanne.yaml</code> or detect git-only mode</li> <li>Determine storage directory</li> <li>Create embedding provider (shared embed server &gt; in-process sentence-transformers &gt; Ollama). The shared server is auto-started if not running and <code>auto_start</code> is enabled.</li> <li>Initialize vectorstore (LanceDB)</li> <li>Start FastAPI server</li> <li>Reconcile \u2014 scan the database, remove entries for files that no longer exist or are now excluded</li> <li>Index \u2014 discover files, skip unchanged (mtime), index the rest</li> <li>Start file watcher</li> </ol> <p>In daemon mode, the server spawns a new process with <code>subprocess.Popen</code> (not fork \u2014 sentence-transformers uses threading internally, and threading + fork don't mix). It writes a PID file and redirects output to a log file.</p>"},{"location":"architecture/#mcp-server-mcp_mainpy","title":"MCP Server (<code>mcp_main.py</code>)","text":"<p>Same initialization, but runs indexing synchronously on startup (needs the index ready before Claude starts querying) and exposes MCP tools over stdio instead of HTTP.</p>"},{"location":"architecture/#the-go-cli","title":"The Go CLI","text":"<p>The CLI (<code>cmd/giddy/main.go</code>) is a thin HTTP client. It doesn't do any indexing or embedding \u2014 it just talks to the Python server.</p>"},{"location":"architecture/#project-discovery","title":"Project Discovery","text":"<p>When you run <code>giddy</code>, it walks up from your current directory looking for: 1. A <code>.giddyanne.yaml</code> file (config mode) 2. A <code>.git/</code> directory (git-only mode)</p> <p>This determines the project root and where to look for the server's storage directory.</p>"},{"location":"architecture/#server-discovery","title":"Server Discovery","text":"<p>The CLI looks for <code>.pid</code> files in the storage directory. The filename encodes the host and port (e.g., <code>-8000.pid</code> for default settings). It reads the PID, checks if the process is alive (<code>kill(pid, 0)</code>), and hits <code>/health</code> to verify.</p>"},{"location":"architecture/#auto-start","title":"Auto-start","text":"<p><code>giddy find</code> will start the server automatically if it's not running. It locates the Python venv relative to the <code>giddy</code> binary's installation directory, spawns the server in daemon mode, and polls <code>/health</code> every 500ms until it's ready.</p>"},{"location":"architecture/#command-matching","title":"Command Matching","text":"<p>Commands can be abbreviated to any unambiguous prefix \u2014 <code>giddy f</code> for <code>find</code>, <code>giddy st</code> for <code>status</code>, etc.</p>"},{"location":"architecture/#storage-layout","title":"Storage Layout","text":"<p>Where files end up depends on whether you have a config file:</p> <p>Config mode (<code>.giddyanne.yaml</code> exists): <pre><code>&lt;project&gt;/.giddyanne/\n\u251c\u2500\u2500 all-MiniLM-L6-v2/     # Named after the embedding model\n\u2502   \u2514\u2500\u2500 vectors.lance/    # LanceDB database\n\u251c\u2500\u2500 -8000.log             # Server log (host-port.log)\n\u2514\u2500\u2500 -8000.pid             # Server PID (host-port.pid)\n</code></pre></p> <p>Git-only mode (no config): <pre><code>$TMPDIR/giddyanne/&lt;project&gt;-&lt;hash&gt;/\n\u2514\u2500\u2500 (same structure)\n</code></pre></p> <p>The hash is the first 8 characters of SHA256 of the absolute project path \u2014 deterministic, so the same project always gets the same storage location.</p> <p>The database directory includes the model name. This means you can switch embedding models without corrupting existing indexes \u2014 each model gets its own database.</p>"},{"location":"architecture/#design-decisions","title":"Design Decisions","text":"<p>Three embeddings per chunk \u2014 path, content, and description each capture different signals. A query like \"test helpers\" benefits from the path embedding matching <code>tests/helpers.py</code>. A query like \"authentication\" benefits from the description embedding if the config says \"auth and session management\". Content embedding does the heavy lifting for everything else.</p> <p>Mtime-based skip \u2014 comparing timestamps is cheap. Re-embedding is expensive. This single optimization makes re-indexing fast enough that you barely notice it on startup.</p> <p>Subprocess, not fork \u2014 sentence-transformers loads PyTorch, which uses threads internally. Forking a threaded process is undefined behavior on most platforms. <code>Popen</code> spawns a clean new process.</p> <p>LanceDB \u2014 it's an embedded database (no separate server process), stores data in Apache Arrow format, and supports both vector search and full-text search. One less thing to install and manage.</p> <p>Batch everything \u2014 the embedding model call is the bottleneck. Batching 32 chunks into a single <code>encode()</code> call is dramatically faster than 32 individual calls. Same principle applies to database upserts.</p> <p>Debounced file watching \u2014 editors often trigger multiple write events for a single save (write to temp file, rename, etc.). The 100ms debounce window catches all of these and fires once.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to giddyanne will be documented in this file.</p>"},{"location":"changelog/#162-2026-02-25","title":"[1.6.2] - 2026-02-25","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Lint fixes: Cleaned up linter warnings in engine and benchmark code</li> </ul>"},{"location":"changelog/#161-2026-02-25","title":"[1.6.1] - 2026-02-25","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Watcher ignores node_modules, .giddyanne, .git, etc.: Hardcoded <code>ALWAYS_IGNORE_DIRS</code> set that is always excluded from watching and indexing regardless of config. Previously, watchdog registered inotify watches on every subdirectory including <code>node_modules</code> (10k+ watches on a typical JS project, now ~140).</li> <li>Non-recursive watcher with selective directory walks: Replaced <code>recursive=True</code> Observer with manual directory walking that prunes ignored dirs. New directories are auto-watched via <code>DirCreatedEvent</code>. Eliminates thousands of wasted kernel inotify watches.</li> <li>Batch upserts in watcher-triggered indexing: <code>index_file</code> (the single-file reindex path) now uses <code>embed_chunks_batch</code> + <code>upsert_batch</code> instead of per-chunk embed/upsert loops. A 22-chunk file previously created 22 LanceDB fragments triggering expensive compaction; now creates 1.</li> <li>Diagnostic logging for memory leaks: RSS tracking via <code>/proc/self/status</code> at every indexing phase, per-file size logging, stats-log output for WATCH/INDEX/PREP/EMBED events visible via <code>giddy log</code>. Watcher <code>fire()</code> callback now catches and logs async exceptions.</li> </ul>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li><code>make qa</code> target: Runs linter (<code>ruff check</code>) and tests (<code>pytest</code>) in one command.</li> <li>GitHub Actions CI: <code>make qa</code> runs automatically on pushes and PRs to <code>main</code>, with venv caching.</li> </ul>"},{"location":"changelog/#160-2026-02-23","title":"[1.6.0] - 2026-02-23","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Emacs Lisp language support: <code>.el</code> files are now indexed with tree-sitter AST chunking, splitting on <code>defun</code>, <code>defmacro</code>, and special forms (<code>defvar</code>, <code>defcustom</code>, etc.)</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Index all project files by default: All files with supported extensions are now indexed automatically, regardless of whether they appear in <code>paths</code> config. The <code>paths</code> config is now description-only \u2014 it adds annotation context that improves search relevance, but doesn't control what gets indexed. <code>.gitignore</code> and <code>ignore_patterns</code> still apply. Watcher covers the entire project root instead of only configured directories.</li> </ul>"},{"location":"changelog/#150-2026-02-22","title":"[1.5.0] - 2026-02-22","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Shared embedding server: A global singleton service (<code>embed_server.py</code>) that loads the embedding model once and serves all project servers over a Unix socket. Project servers that previously loaded their own ~900MB model copy now proxy through the shared server at ~180MB each. Auto-starts on <code>giddy up</code> if not already running. Manage directly with <code>giddy embed start|stop|status</code>. Configurable via <code>~/.config/giddyanne/config.yaml</code> (socket path, model, auto_start). Falls back to in-process embedding if the shared server is unavailable.</li> <li><code>giddy embed</code> CLI command: <code>giddy embed start</code>, <code>giddy embed stop</code>, <code>giddy embed status</code> for managing the shared embedding server independently of any project.</li> <li>Global config (<code>~/.config/giddyanne/config.yaml</code>): Optional config for the shared embedding service \u2014 socket path, PID/log paths, model name, and auto_start toggle.</li> <li>Cross-encoder reranker (C4): Optional second-stage reranker using <code>cross-encoder/ms-marco-MiniLM-L6-v2</code> (22.7M params). Over-fetches candidates (5x instead of 3x), runs existing pipeline (RRF, category bias, dedup), then re-scores with the cross-encoder before returning. Opt-in via <code>reranker_model</code> in <code>.giddyanne.yaml</code>. Uses <code>CrossEncoder</code> from sentence-transformers (no new dependency). Benchmarks: giddyanne recall 35%\u219280% (hybrid), MRR 0.34\u21920.90. test_2 FTS recall 60%\u219270%, MRR 0.40\u21920.70. test_1 MRR improved (+0.05-0.15) but semantic recall regressed 87%\u219267% on one query where the right file wasn't in the candidate pool. Disabled by default \u2014 best for repos where relevant files are retrieved but poorly ranked.</li> <li><code>--rerank</code> CLI flag: Enable the cross-encoder reranker without a config file. <code>giddy up --rerank</code> and <code>giddy bounce --rerank</code> apply the default reranker model. For MCP, set <code>GIDDY_RERANK=1</code> environment variable.</li> <li>Token-aware chunk splitting: After language-aware chunking, oversized chunks are recursively halved until they fit the model's token budget (256 tokens for MiniLM). Eliminates silent embedding truncation \u2014 content coverage went from 56% to 100%. Chunk count roughly doubles (313\u2192740 for giddyanne repo). Benchmarks: test_2 recall 70%\u219277%, MRR 0.70\u21920.85. Semantic-only search now matches old hybrid baseline. Index times ~2x from more chunks. Ollama backend now impractical without batching (timed out at 600s on 32 files due to per-request HTTP overhead \u00d7 ~2200 embed calls).</li> <li>Ollama embedding backend: GPU-accelerated embeddings via a local Ollama server. Opt-in via <code>--ollama</code> CLI flag, <code>GIDDY_OLLAMA=1</code> env var, or <code>ollama: true</code> in config. Configurable URL and model (<code>ollama_url</code>, <code>ollama_model</code>). Default model: <code>all-minilm:l6-v2</code>.</li> <li>Truncation stats: <code>/stats</code> endpoint and <code>giddy stats</code> now report embedding coverage \u2014 how many chunks were truncated and what percentage of content is fully covered.</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>BM25 field boosting (C3): Added <code>fts_content</code> column that repeats file path and symbol name 3x before raw content, simulating BM25F field-level boosting. FTS now indexes this enriched column instead of raw content. Benchmarks: test_1 full-text recall 63%\u219287%, \"grid\" query ranks <code>grid.tsx</code> #1. test_2 Stripe query ranks <code>stripe.ts</code> #1. giddyanne hybrid MRR holds at 1.00. Requires <code>giddy clean --force &amp;&amp; giddy up</code> to rebuild index.</li> <li>Chunk context enrichment (C1): Prepend file path and function/class name to chunk content before embedding. Stored content unchanged (search results still show raw code). Benchmarks: giddyanne 75% recall (steady, \"configuration and settings\" query improved 0%\u219250%), test_2 80%\u219287%, test_1 77%\u219270% (one already-weak query regressed).</li> <li>Custom RRF with tunable weights (C2): Replaced LanceDB's black-box <code>RRFReranker</code> with our own weighted Reciprocal Rank Fusion. Runs semantic and FTS searches independently, merges ranked lists with configurable weights (semantic 1.2, FTS 0.5). Removes <code>lancedb.rerankers</code> dependency from hybrid search. Benchmarks: recall matches semantic-only (giddyanne 75%, test_2 70%, test_1 83%), MRR improved (giddyanne 0.65\u21921.00, test_2 0.65\u21920.70).</li> </ul>"},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Quality benchmark script (<code>benchmarks/quality.py</code>): Reusable benchmark for measuring search recall, precision, and MRR across projects and search modes. Supports <code>--mode all</code> for side-by-side comparison of semantic, full-text, and hybrid search.</li> </ul>"},{"location":"changelog/#141-2026-02-21","title":"[1.4.1] - 2026-02-21","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Version references: Updated all version strings across docs, Emacs package, and VSCode extension</li> </ul>"},{"location":"changelog/#140-2026-02-21","title":"[1.4.0] - 2026-02-21","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Search result deduplication: Results are now deduplicated by file \u2014 only the best-scoring chunk per file is kept, freeing result slots for diverse files. Applied to all three search methods (semantic, full-text, hybrid). Over-fetches from LanceDB to compensate for collapsed results.</li> <li>File category scoring bias: Search scores are now weighted by file type \u2014 source code files rank at full weight (1.0\u00d7), test files at 0.8\u00d7, and docs at 0.6\u00d7. When a test file and its source have similar relevance, the source file surfaces first.</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Tree-sitter AST chunking: Replaced string-based separator splitting with tree-sitter AST parsing for code chunking. Functions, classes, and other definitions are now split at real node boundaries instead of pattern-matching on strings like <code>\"\\ndef \"</code>. Falls back to blank-line splitting for languages without a tree-sitter grammar. Adds <code>tree-sitter-language-pack</code> dependency.</li> <li>Configurable embedding model: <code>local_model</code> in <code>.giddyanne.yaml</code> lets you swap embedding models. Each model gets its own index directory. Default remains <code>all-MiniLM-L6-v2</code> \u2014 benchmarking showed <code>nomic-ai/CodeRankEmbed</code> (768d, code-specialized) indexes 5-6x slower and can't finish indexing a modest ~300-file repo on a MacBook Pro, with no quality gain to justify it.</li> <li>Cosine distance for vector search: Switched from L2 to cosine distance. Scores now display as meaningful 0-1 values instead of near-zero floats.</li> <li>Query prefix support: Models that require instruction prefixes on search queries (like CodeRankEmbed) are handled automatically. Document embeddings are unaffected.</li> <li><code>giddy install</code> reads model from config: No longer hardcodes the model name \u2014 picks up <code>local_model</code> from <code>.giddyanne.yaml</code> or uses the default.</li> </ul>"},{"location":"changelog/#132-2026-02-20","title":"[1.3.2] - 2026-02-20","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>MCP status/health/stats tools: AI assistants can now check indexing progress, server health, and index statistics directly via MCP</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li><code>giddy find</code> waits for indexing on cold start: Previously returned a 503 error and exited if the server was still indexing. Now silently waits until ready, then returns results. Use <code>--verbose</code> to see progress.</li> </ul>"},{"location":"changelog/#131-2026-02-20","title":"[1.3.1] - 2026-02-20","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Sitemap command in Emacs and VSCode: <code>giddyanne-sitemap</code> (Emacs) and <code>Giddyanne: Sitemap</code> (VSCode) show indexed files as a tree</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Fix empty tree node for standalone file path groups in MCP sitemap output</li> <li>Fix unused variable warning in MCP server entrypoint</li> <li>Fix <code>fmt.Println</code> redundant newline warning in Go CLI</li> </ul>"},{"location":"changelog/#130-2026-02-20","title":"[1.3.0] - 2026-02-20","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>Sitemap command: new <code>/sitemap</code> endpoint, MCP <code>sitemap</code> tool, and <code>giddy sitemap</code> CLI command. Shows all indexed files, optionally with chunk counts and modification times (<code>--verbose</code>). API response includes a <code>paths</code> field with <code>path</code> and <code>description</code> for each configured group.</li> <li>File-tree sitemap output: sitemap displays files under their path groups with <code>\u251c\u2500\u2500</code>/<code>\u2514\u2500\u2500</code> tree connectors, intermediate directories reconstructed. Verbose mode appends chunk count and mtime inline.</li> <li>Sitemap API tests: test coverage for <code>/sitemap</code> endpoint with and without config, including verbose mode and empty index</li> </ul>"},{"location":"changelog/#121-2026-02-12","title":"[1.2.1] - 2026-02-12","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li><code>giddy install</code> command: Downloads the embedding model (~90MB) upfront so <code>giddy up</code> starts immediately without a silent download pause</li> </ul>"},{"location":"changelog/#120-2026-02-12","title":"[1.2.0] - 2026-02-12","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li><code>--version</code> / <code>-V</code> flag for CLI (<code>giddy --version</code> prints <code>giddy 1.2.0</code>)</li> <li>MCP <code>version</code> tool returns the installed giddyanne version</li> <li>Single version source of truth: <code>pyproject.toml</code> drives all version strings \u2014 Go CLI reads it at build time via ldflags, Python reads via <code>importlib.metadata</code></li> <li>Zero-config git repo support: <code>giddy</code> now works in any git repo without a <code>.giddyanne.yaml</code> config file. Auto-detects common source directories (<code>src/</code>, <code>lib/</code>, <code>app/</code>, <code>tests/</code>, <code>docs/</code>) and stores indexes in the system temp directory. Config file still takes priority when present.</li> </ul>"},{"location":"changelog/#111-2025-12-31","title":"[1.1.1] - 2025-12-31","text":"<p>Installation improvements.</p>"},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>Pre-built binaries for macOS (arm64, amd64) and Linux (amd64) attached to GitHub releases</li> <li>Flexible install location: <code>make install BIN_DIR=/usr/local/bin</code></li> </ul>"},{"location":"changelog/#110-2025-12-31","title":"[1.1.0] - 2025-12-31","text":"<p>Search quality improvements, Emacs integration, and CLI polish.</p>"},{"location":"changelog/#added_11","title":"Added","text":"<p>Search Quality - Language-aware chunking: split code at function/class boundaries using language-specific separators - Hybrid search: BM25 + semantic with <code>--semantic</code>, <code>--full-text</code>, <code>--hybrid</code> (default) modes - Model-specific vector stores: database path includes embedding model name - Faster initial import: parallelized file I/O + batched encode() calls</p> <p>CLI - Shell completions for bash, zsh, and fish - Health metrics: index size, query latency via <code>/health</code> endpoint and <code>giddy health</code> - Prefix shortcuts: <code>giddy f</code> for <code>find</code>, <code>giddy st</code> for <code>status</code>, etc. - Better startup experience: friendlier first-run, clearer server feedback</p> <p>Emacs - Full integration package with Vertico support and nerd-icons - Commands: <code>giddyanne-find</code>, <code>giddyanne-up/down</code>, <code>giddyanne-status</code>, <code>giddyanne-health</code>, <code>giddyanne-log</code> - Results grouped by file with pulse highlight on jump - Installation docs for Doom Emacs</p> <p>VSCode - Extension with QuickPick search UI - Commands: Find, Start/Stop Server, Status, Health, Stream Logs - Results grouped by file with pulse highlight on jump - Keybinding: <code>Cmd+Shift+G</code> / <code>Ctrl+Shift+G</code> - Build with <code>make vscode</code>, install via <code>.vsix</code></p> <p>Languages - Language support expanded from 6 to 28 languages:   - Programming: C, C++, C#, Dart, Elixir, Java, Kotlin, Lua, Objective-C, PHP, R, Ruby, Scala, Shell, Swift, Zig   - Markup &amp; config: CSS, HTML, JSON, Markdown, TOML, YAML</p>"},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>INFO logs no longer appear without <code>--verbose</code> flag</li> <li><code>giddy health</code> shows current state when server is down (like <code>giddy status</code>)</li> <li><code>giddy clean</code> finds model-specific index paths</li> <li><code>--files-only --json</code> returns only path and lines, not content</li> </ul>"},{"location":"changelog/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Database path includes model name. Migration: <code>mkdir -p .giddyanne/all-MiniLM-L6-v2 &amp;&amp; mv .giddyanne/vectors.lance .giddyanne/all-MiniLM-L6-v2/</code></li> <li>Removed OpenRouter support. Remove <code>GIDDY_OPENROUTER_API_KEY</code> env var and <code>openrouter_model</code> from config</li> </ul>"},{"location":"changelog/#100-2025-12-30","title":"[1.0.0] - 2025-12-30","text":"<p>First stable release. Semantic code search for codebases via CLI and MCP server.</p>"},{"location":"changelog/#features","title":"Features","text":"<p>Core - Semantic code search using local sentence-transformers embeddings - LanceDB vector store with chunk-based schema - Hybrid chunking: split on blank lines, merge small chunks, split large with overlap - File watcher for automatic re-indexing on changes - Search query caching with hit statistics - File timestamp tracking to skip unchanged files on startup - Index reconciliation removes stale files on startup - Glob patterns in ignore settings</p> <p>CLI (<code>giddy</code>) - <code>giddy search &lt;query&gt;</code> - search with auto-start if server not running - <code>giddy start</code> - start the server daemon - <code>giddy stop</code> - stop the server - <code>giddy status</code> - show server status and indexing progress - <code>giddy monitor</code> - stream server logs - <code>giddy clean</code> - clear the search index - <code>giddy init</code> - generate config file prompt - <code>giddy mcp</code> - run MCP server for Claude Code - <code>--verbose</code> flag for debug logging - <code>--port</code>, <code>--host</code> flags for server configuration</p> <p>HTTP API - <code>POST /search</code> - semantic search - <code>GET /status</code> - indexing progress - <code>GET /health</code> - liveness check - <code>GET /stats</code> - index statistics</p> <p>MCP Server - Single <code>search</code> tool for Claude Code integration - Description embeddings for better context matching</p> <p>Configuration - <code>.giddyanne.yaml</code> project config with paths and descriptions - Configurable chunking parameters</p>"},{"location":"cli/","title":"CLI Reference","text":"<p>Run <code>giddy help</code> for built-in help.</p>"},{"location":"cli/#commands","title":"Commands","text":"Command Description <code>giddy find &lt;query&gt;</code> Search the codebase (auto-starts server if needed) <code>giddy up</code> Start the server <code>giddy down</code> Stop the server <code>giddy bounce</code> Restart the server <code>giddy status</code> Show server status and indexing progress <code>giddy health</code> Show index statistics (files, chunks, size) <code>giddy sitemap</code> List all indexed files <code>giddy log</code> Stream server logs in real-time <code>giddy drop</code> Remove search index (keeps logs) <code>giddy clean</code> Remove all giddyanne data (with confirmation) <code>giddy init</code> Generate a config file prompt <code>giddy embed &lt;sub&gt;</code> Manage the shared embedding server (<code>start</code>, <code>stop</code>, <code>status</code>) <code>giddy mcp</code> Run MCP server (for Claude Code)"},{"location":"cli/#abbreviations","title":"Abbreviations","text":"<p>Commands can be shortened to their prefix:</p> Short Full <code>giddy f</code> <code>giddy find</code> <code>giddy st</code> <code>giddy status</code> <code>giddy h</code> <code>giddy health</code> <code>giddy si</code> <code>giddy sitemap</code> <code>giddy l</code> <code>giddy log</code> <code>giddy b</code> <code>giddy bounce</code>"},{"location":"cli/#flags","title":"Flags","text":""},{"location":"cli/#server-flags","title":"Server flags","text":"<pre><code>giddy up --port 9000      # Use specific port\ngiddy up --host localhost  # Bind to specific host\ngiddy up --rerank          # Enable cross-encoder reranking\ngiddy up --ollama          # Use Ollama for embeddings (GPU-accelerated)\ngiddy up --verbose         # Enable debug logging\n</code></pre>"},{"location":"cli/#search-flags","title":"Search flags","text":"<pre><code>giddy find --limit 20     # Return more results (default: 10)\ngiddy find --json         # Output as JSON\ngiddy find --files-only   # Only show file paths\ngiddy find --verbose      # Show full content (no truncation)\ngiddy find --semantic     # Semantic search only\ngiddy find --full-text    # Full-text search only\n</code></pre>"},{"location":"cli/#embed-flags","title":"Embed flags","text":"<pre><code>giddy embed start             # Start the shared embedding server\ngiddy embed start --verbose   # With debug logging\ngiddy embed start --model X   # Use a specific model\ngiddy embed stop              # Stop the shared embedding server\ngiddy embed status            # Show PID and loaded models\n</code></pre>"},{"location":"cli/#other-flags","title":"Other flags","text":"<pre><code>giddy --version           # Print version and exit\ngiddy -V                  # Short form\ngiddy health --verbose    # List all files with chunk counts\ngiddy sitemap             # List all indexed file paths\ngiddy sitemap --verbose   # Include chunk counts and modification times\ngiddy sitemap --json      # Output as JSON\ngiddy clean --force       # Skip confirmation prompt\n</code></pre>"},{"location":"cli/#shell-completions","title":"Shell Completions","text":"<p>Enable tab completion for commands and flags.</p> <p>Bash (add to <code>~/.bashrc</code>): <pre><code>eval \"$(giddy completion bash)\"\n</code></pre></p> <p>Zsh (add to <code>~/.zshrc</code>): <pre><code>eval \"$(giddy completion zsh)\"\n</code></pre></p> <p>Fish (add to <code>~/.config/fish/config.fish</code>): <pre><code>giddy completion fish | source\n</code></pre></p> <p></p>"},{"location":"config/","title":"Configuration","text":"<p>Giddyanne works in any git repo without configuration. A <code>.giddyanne.yaml</code> file is optional but recommended for better search quality.</p>"},{"location":"config/#zero-config-mode","title":"Zero-Config Mode","text":"<p>In a git repo without <code>.giddyanne.yaml</code>, giddyanne automatically: - Uses the git root as the project root - Indexes common directories (<code>src/</code>, <code>lib/</code>, <code>app/</code>, <code>tests/</code>, <code>docs/</code>) or the whole project if none exist - Respects <code>.gitignore</code> patterns - Stores indexes in the system temp directory (<code>$TMPDIR/giddyanne/&lt;name&gt;-&lt;hash&gt;/</code>)</p> <p>Indexes rebuild automatically if the temp directory gets cleaned. This is fine \u2014 <code>giddy up</code> is fast.</p>"},{"location":"config/#config-file","title":"Config File","text":"<p>For better search quality, add a <code>.giddyanne.yaml</code> to your project root. Path descriptions are embedded alongside content, improving semantic matching.</p>"},{"location":"config/#minimal-config","title":"Minimal Config","text":"<pre><code>paths:\n  - path: .\n    description: My app\n</code></pre> <p>That's it. Only supported language files are indexed, and <code>.gitignore</code> is respected automatically.</p>"},{"location":"config/#annotated-example","title":"Annotated Example","text":"<pre><code>paths:\n  - path: src/\n    description: Core application source code\n\n  - path: src/auth/\n    description: Authentication, login, sessions, permissions\n\n  - path: tests/\n    description: Test suite and fixtures\n</code></pre> <p>Descriptions are embedded alongside content for better semantic matching. More specific descriptions improve search quality.</p>"},{"location":"config/#generating-a-config","title":"Generating a Config","text":"<p>Run <code>giddy init</code> to get a prompt you can paste into Claude or another LLM:</p> <pre><code>claude \"follow the instructions: $(giddy init)\"\n</code></pre> <p>The LLM will analyze your codebase and generate an appropriate config.</p>"},{"location":"config/#full-example","title":"Full Example","text":"<p>All available options:</p> <pre><code>paths:\n  # Index the entire project with a general description\n  - path: .\n    description: Web application with REST API\n\n  # Override with more specific descriptions for key directories\n  - path: src/api/\n    description: REST endpoints, request handlers, middleware\n\n  - path: src/auth/\n    description: Authentication, JWT tokens, session management, permissions\n\n  - path: src/db/\n    description: Database models, migrations, query builders\n\n  - path: src/utils/\n    description: Shared utilities, helpers, validation functions\n\n  - path: tests/\n    description: Test suite, fixtures, mocks\n\nsettings:\n  # File handling\n  max_file_size: 1000000           # Skip files &gt; 1MB\n  ignore_patterns:                  # Beyond .gitignore\n    - \"*.generated.*\"\n    - \"*.min.js\"\n    - \"vendor/\"\n\n  # Server\n  host: \"0.0.0.0\"                  # Bind address\n  port: 8000                        # HTTP port\n\n  # Chunking (defaults work well for most codebases)\n  min_chunk_lines: 10\n  max_chunk_lines: 50\n  overlap_lines: 5\n\n  # Embedding model\n  local_model: \"all-MiniLM-L6-v2\"\n\n  # Ollama backend (optional, disabled by default)\n  ollama: true\n  ollama_url: \"http://localhost:11434\"\n  ollama_model: \"all-minilm:l6-v2\"\n\n  # Reranker (optional, disabled by default)\n  reranker_model: \"cross-encoder/ms-marco-MiniLM-L6-v2\"\n</code></pre>"},{"location":"config/#settings-reference","title":"Settings Reference","text":""},{"location":"config/#paths","title":"Paths","text":"Field Required Description <code>path</code> Yes File or directory to index (relative to config file) <code>description</code> No Human-readable description, embedded alongside content for better search"},{"location":"config/#file-handling","title":"File Handling","text":"Setting Default Description <code>max_file_size</code> 1000000 Skip files larger than this (bytes) <code>ignore_patterns</code> [] Additional patterns to ignore (uses gitignore syntax)"},{"location":"config/#server","title":"Server","text":"Setting Default Description <code>host</code> \"0.0.0.0\" Network interface to bind to <code>port</code> 8000 HTTP port for the server"},{"location":"config/#chunking","title":"Chunking","text":"<p>Code is split into chunks for indexing. These settings control chunk boundaries:</p> Setting Default Description <code>min_chunk_lines</code> 10 Minimum lines per chunk <code>max_chunk_lines</code> 50 Maximum lines per chunk <code>overlap_lines</code> 5 Lines of overlap between chunks <p>Defaults work well for most codebases. Smaller chunks give more precise results but may lose context.</p>"},{"location":"config/#embedding","title":"Embedding","text":"Setting Default Description <code>local_model</code> \"all-MiniLM-L6-v2\" Sentence-transformers model name <p>With a config file, the database is stored at <code>.giddyanne/&lt;model-name&gt;/vectors.lance</code>. In zero-config mode, it's stored in the system temp directory. Each model gets its own index, so you can switch models without losing data.</p> <p>Available models: - <code>all-MiniLM-L6-v2</code> (default) \u2014 Good balance of speed and quality - <code>all-mpnet-base-v2</code> \u2014 Higher quality, slower - <code>paraphrase-MiniLM-L3-v2</code> \u2014 Faster, lower quality - <code>nomic-ai/CodeRankEmbed</code> \u2014 Code-specialized (768d), but 5-6x slower indexing with no measurable quality gain. Unusable on laptop hardware for repos over ~100 files.</p>"},{"location":"config/#ollama","title":"Ollama","text":"<p>Use a local Ollama server for GPU-accelerated embeddings instead of the default sentence-transformers (CPU).</p> Setting Default Description <code>ollama</code> false Enable Ollama backend <code>ollama_url</code> \"http://localhost:11434\" Ollama server URL <code>ollama_model</code> \"all-minilm:l6-v2\" Ollama embedding model <pre><code>settings:\n  ollama: true\n  ollama_model: \"all-minilm:l6-v2\"\n</code></pre> <p>Without a config file, use the <code>--ollama</code> CLI flag instead:</p> <pre><code>giddy up --ollama\n</code></pre> <p>For MCP, set the <code>GIDDY_OLLAMA=1</code> environment variable in your MCP config.</p> <p>Requires Ollama running locally with the model pulled (<code>ollama pull all-minilm:l6-v2</code>).</p>"},{"location":"config/#reranker","title":"Reranker","text":"Setting Default Description <code>reranker_model</code> \"\" (disabled) Cross-encoder model for result reranking <p>The reranker is a second-stage scorer that re-evaluates the top search results using a cross-encoder. It improves ranking quality at the cost of slightly slower queries (the model scores each result individually).</p> <pre><code>settings:\n  reranker_model: \"cross-encoder/ms-marco-MiniLM-L6-v2\"\n</code></pre> <p>Without a config file, use the <code>--rerank</code> CLI flag instead:</p> <pre><code>giddy up --rerank\n</code></pre> <p>For MCP, set the <code>GIDDY_RERANK=1</code> environment variable in your MCP config.</p>"},{"location":"config/#global-config","title":"Global Config","text":"<p>The shared embedding server has its own config at <code>~/.config/giddyanne/config.yaml</code>. This is separate from per-project <code>.giddyanne.yaml</code> files.</p> <pre><code># All fields are optional \u2014 defaults shown\nembed_model: \"all-MiniLM-L6-v2\"\nauto_start: true\nsocket_path: \"~/.local/state/giddyanne/embed.sock\"\npid_path: \"~/.local/state/giddyanne/embed.pid\"\nlog_path: \"~/.local/state/giddyanne/embed.log\"\n</code></pre> Setting Default Description <code>embed_model</code> \"all-MiniLM-L6-v2\" Model the shared server loads <code>auto_start</code> true Auto-start the embed server when a project server starts <code>socket_path</code> <code>~/.local/state/giddyanne/embed.sock</code> Unix socket for embed server <code>pid_path</code> <code>~/.local/state/giddyanne/embed.pid</code> PID file location <code>log_path</code> <code>~/.local/state/giddyanne/embed.log</code> Log file location <p>Set <code>auto_start: false</code> to disable the shared server and fall back to in-process embedding (each project loads its own model copy).</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions welcome!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code>git clone https://github.com/upship-ai/giddyanne.git\ncd giddyanne\n\n# Build everything\nmake\n\n# Install dev dependencies\n.venv/bin/pip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Linter\n.venv/bin/ruff check .\n\n# Tests\n.venv/bin/pytest\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>giddyanne/\n\u251c\u2500\u2500 cmd/giddy/main.go    # Go CLI\n\u251c\u2500\u2500 http_main.py         # Python HTTP server\n\u251c\u2500\u2500 mcp_main.py          # Python MCP server\n\u251c\u2500\u2500 src/                 # Python source (engine, startup, chunker, embeddings, vectorstore, etc.)\n\u251c\u2500\u2500 emacs/               # Emacs package\n\u2514\u2500\u2500 vscode/              # VSCode extension\n</code></pre> <p>For how these pieces fit together \u2014 the indexing pipeline, search internals, file watching, and design decisions \u2014 see Architecture.</p>"},{"location":"contributing/#help-wanted","title":"Help Wanted","text":"<p>Here's what would help most:</p>"},{"location":"contributing/#windows-support","title":"Windows Support","text":"<p>The Python server should work, but needs testing and documentation: - Test installation and server on Windows - Document any needed changes (paths, process management) - Update Makefile or add Windows-specific install script</p>"},{"location":"contributing/#vimneovim-plugin","title":"Vim/Neovim Plugin","text":"<p>A plugin that calls the CLI and populates quickfix: - <code>giddy find</code> integration with fzf or telescope - Results to quickfix list - Basic commands: up, down, status</p> <p>Open an issue to discuss or submit a PR.</p>"},{"location":"install/","title":"Installation","text":"<p>Platforms: macOS, Linux</p>"},{"location":"install/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Go 1.21+ (or use pre-built binary)</li> </ul>"},{"location":"install/#build-from-source","title":"Build from Source","text":"<pre><code>git clone --depth 1 --branch v1.6.2 https://github.com/upship-ai/giddyanne.git\ncd giddyanne\nmake install\n</code></pre> <p>This builds the CLI, creates a Python virtualenv, and symlinks <code>giddy</code> to <code>~/bin</code>.</p> <p>To install elsewhere:</p> <pre><code>make install BIN_DIR=/usr/local/bin  # or any directory in your PATH\n</code></pre>"},{"location":"install/#pre-built-binary","title":"Pre-built Binary","text":"<p>If you don't have Go installed:</p> <pre><code># Clone the repo (needed for Python server code)\ngit clone --depth 1 --branch v1.6.2 https://github.com/upship-ai/giddyanne.git\ncd giddyanne\n\n# Download binary for your platform from Releases:\n# https://github.com/upship-ai/giddyanne/releases\n# Move it into the repo root (same directory as http_main.py)\n\n# On macOS, clear the quarantine flag\nxattr -d com.apple.quarantine giddy\n\n# Set up Python environment\nmake python\n\n# Symlink to PATH (preserves path to Python code)\nln -sf \"$(pwd)/giddy\" ~/bin/giddy\n</code></pre>"},{"location":"install/#first-run","title":"First Run","text":"<p>First install downloads dependencies (~2GB for PyTorch). This is a one-time download.</p> <p>Then download the embedding model:</p> <pre><code>giddy install      # Download embedding model (~90MB, one-time)\n</code></pre> <p>If you skip this step, the model downloads automatically on first <code>giddy up</code> \u2014 but the explicit install step makes the wait visible instead of silently blocking startup.</p> <p>Initial indexing takes time depending on codebase size: - ~750 files: ~45 seconds on M1 Pro - Runs once at startup, then watches for changes - Subsequent starts are faster (unchanged files use cached embeddings)</p>"},{"location":"install/#verify-installation","title":"Verify Installation","text":"<pre><code>giddy help\n# Shows available commands\n</code></pre>"},{"location":"install/#uninstall","title":"Uninstall","text":"<pre><code># Remove symlink\nrm ~/bin/giddy  # or wherever you installed it\n\n# Remove the repo\nrm -rf /path/to/giddyanne\n\n# Optional: remove cached models\nrm -rf ~/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2\n</code></pre>"},{"location":"integrations/","title":"Integrations","text":""},{"location":"integrations/#mcp-server-claude-code","title":"MCP Server (Claude Code)","text":"<p>Add to your project's <code>.mcp.json</code> (or global <code>~/.claude/config.json</code>):</p> <pre><code>{\n  \"mcpServers\": {\n    \"giddyanne\": {\n      \"command\": \"giddy\",\n      \"args\": [\"mcp\"]\n    }\n  }\n}\n</code></pre> <p>The <code>giddy mcp</code> command finds the project root by walking up to <code>.giddyanne.yaml</code>, or falls back to the nearest <code>.git/</code> directory.</p> <p>Optional environment variables for MCP:</p> Variable Description <code>GIDDY_RERANK=1</code> Enable cross-encoder reranking <code>GIDDY_OLLAMA=1</code> Use Ollama for GPU-accelerated embeddings <pre><code>{\n  \"mcpServers\": {\n    \"giddyanne\": {\n      \"command\": \"giddy\",\n      \"args\": [\"mcp\"],\n      \"env\": { \"GIDDY_RERANK\": \"1\", \"GIDDY_OLLAMA\": \"1\" }\n    }\n  }\n}\n</code></pre>"},{"location":"integrations/#emacs","title":"Emacs","text":"<p>Requires Emacs 28.1+.</p> <p>Add to your init file:</p> <pre><code>(add-to-list 'load-path \"/path/to/giddyanne/emacs\")  ; adjust to your clone location\n(require 'giddyanne)\n\n;; Optional keybindings\n(global-set-key (kbd \"C-c g f\") #'giddyanne-find)\n(global-set-key (kbd \"C-c g u\") #'giddyanne-up)\n(global-set-key (kbd \"C-c g d\") #'giddyanne-down)\n(global-set-key (kbd \"C-c g s\") #'giddyanne-status)\n</code></pre> <p>Or with <code>use-package</code>:</p> <pre><code>(use-package giddyanne\n  :load-path \"/path/to/giddyanne/emacs\"\n  :bind ((\"C-c g f\" . giddyanne-find)\n         (\"C-c g u\" . giddyanne-up)\n         (\"C-c g d\" . giddyanne-down)\n         (\"C-c g s\" . giddyanne-status)))\n</code></pre> Doom Emacs config <pre><code>(use-package giddyanne\n  :load-path \"/path/to/giddyanne/emacs\"\n  :commands (giddyanne-find giddyanne-up giddyanne-down giddyanne-status giddyanne-log giddyanne-health)\n  :init\n  (map! :leader\n        (:prefix \"s\"\n         :desc \"giddyanne\" \"g\" #'giddyanne-find)\n        (:prefix \"g\"\n         (:prefix (\"a\" . \"giddyanne\")\n          :desc \"up\" \"u\" #'giddyanne-up\n          :desc \"down\" \"d\" #'giddyanne-down\n          :desc \"find\" \"f\" #'giddyanne-find\n          :desc \"log\" \"l\" #'giddyanne-log\n          :desc \"status\" \"s\" #'giddyanne-status\n          :desc \"health\" \"h\" #'giddyanne-health))))\n</code></pre>"},{"location":"integrations/#commands","title":"Commands","text":"Command Description <code>giddyanne-find</code> Semantic search with completion <code>giddyanne-up</code> Start server <code>giddyanne-down</code> Stop server <code>giddyanne-status</code> Show server status <code>giddyanne-health</code> Show index stats <code>giddyanne-sitemap</code> Show indexed files as tree <code>giddyanne-log</code> Toggle log buffer <p>The package integrates with Vertico (showing results grouped by file) and nerd-icons (file type icons in results).</p> <p></p>"},{"location":"integrations/#vscode","title":"VSCode","text":"<p>Requires Node.js 18+ and npm.</p> <pre><code>cd giddyanne\nmake vscode\ncode --install-extension vscode/giddyanne-1.6.2.vsix\n</code></pre>"},{"location":"integrations/#commands_1","title":"Commands","text":"Command Keybinding Description <code>Giddyanne: Find</code> <code>Cmd+Shift+G</code> Semantic search with QuickPick <code>Giddyanne: Start Server</code> Start server <code>Giddyanne: Stop Server</code> Stop server <code>Giddyanne: Server Status</code> Show server status <code>Giddyanne: Index Health</code> Show index stats <code>Giddyanne: Sitemap</code> Show indexed files as tree <code>Giddyanne: Stream Logs</code> Open terminal with live logs"},{"location":"integrations/#http-api","title":"HTTP API","text":"<p>The CLI manages the HTTP server for you, but you can also use the API directly:</p> <pre><code># Search files\ncurl -X POST http://localhost:8000/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"database connection\", \"limit\": 5}'\n\n# Get stats\ncurl http://localhost:8000/stats\n\n# Check indexing progress\ncurl http://localhost:8000/status\n\n# Health check\ncurl http://localhost:8000/health\n</code></pre>"},{"location":"integrations/#endpoints","title":"Endpoints","text":"Endpoint Method Description <code>/search</code> POST Search the codebase. Body: <code>{\"query\": \"...\", \"limit\": 10}</code> <code>/status</code> GET Indexing progress and server state <code>/stats</code> GET Index statistics (files, chunks, size) <code>/health</code> GET Liveness check <code>/sitemap</code> GET List all indexed file paths. <code>?verbose=true</code> for chunk counts and mtimes"},{"location":"searching/","title":"Searching","text":"<p>How to get the most out of giddyanne's semantic search.</p>"},{"location":"searching/#how-it-works","title":"How It Works","text":"<p>Giddyanne uses embedding models to understand the meaning of code, not just match keywords:</p> <ul> <li>\"auth logic\" finds <code>login.py</code>, <code>session_manager.py</code>, <code>permissions.go</code></li> <li>\"error handling\" finds try/catch blocks, error types, recovery code</li> <li>\"database queries\" finds SQL, ORM calls, connection pooling</li> </ul> <p>The search combines two approaches: - Semantic search: Finds code with similar meaning to your query - Full-text search: Finds exact matches for specific terms</p> <p>By default, results use both (hybrid search). Use <code>--semantic</code> or <code>--full-text</code> to use only one.</p>"},{"location":"searching/#writing-good-queries","title":"Writing Good Queries","text":""},{"location":"searching/#what-works-well","title":"What Works Well","text":"<p>Conceptual queries - describe what the code does: <pre><code>giddy find \"user authentication\"\ngiddy find \"parse command line arguments\"\ngiddy find \"send email notifications\"\n</code></pre></p> <p>Problem-oriented queries - describe what you're trying to solve: <pre><code>giddy find \"handle network timeouts\"\ngiddy find \"validate user input\"\ngiddy find \"cache expensive computations\"\n</code></pre></p> <p>Behavior descriptions - describe what happens: <pre><code>giddy find \"retry failed requests\"\ngiddy find \"log errors to file\"\ngiddy find \"convert dates to timestamps\"\n</code></pre></p>"},{"location":"searching/#what-works-less-well","title":"What Works Less Well","text":"<p>Variable names - use grep instead: <pre><code># Less effective:\ngiddy find \"userAuthToken\"\n\n# Better:\ngrep -r \"userAuthToken\" src/\n</code></pre></p> <p>Very short queries - add context: <pre><code># Too vague:\ngiddy find \"config\"\n\n# Better:\ngiddy find \"load configuration from yaml file\"\n</code></pre></p>"},{"location":"searching/#tips","title":"Tips","text":"<ol> <li>Use natural language - write queries like you'd describe the code to someone</li> <li>Be specific - \"database connection pooling\" beats \"database stuff\"</li> <li>Include verbs - \"validate\", \"parse\", \"send\", \"handle\" help semantic matching</li> <li>Add context - \"http error handling\" is better than just \"errors\"</li> </ol>"},{"location":"searching/#search-modes","title":"Search Modes","text":"<pre><code>giddy find \"auth\"              # Hybrid (default) - combines both approaches\ngiddy find \"auth\" --semantic   # Semantic only - meaning-based matching\ngiddy find \"auth\" --full-text  # Full-text only - keyword matching\n</code></pre> <p>When to use each: - Hybrid: Best for most queries - Semantic: When you're describing functionality conceptually - Full-text: When you know specific terms that should appear in the code</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"troubleshooting/#no-results-found","title":"\"No results found\"","text":"<ol> <li>Check server status: Run <code>giddy status</code> - is indexing complete?</li> <li>Rephrase your query: Try describing what the code does differently</li> <li>Check paths: Does your config include the directory you're searching?</li> <li>Try hybrid search: If using <code>--semantic</code> or <code>--full-text</code>, try without flags</li> </ol>"},{"location":"troubleshooting/#no-giddyanneyaml-or-git-repository-found","title":"\"No .giddyanne.yaml or git repository found\"","text":"<p>Giddyanne needs either a <code>.giddyanne.yaml</code> config file or a <code>.git/</code> directory in the current directory or any parent.</p> <p>If you're in a git repo, this should work automatically. Otherwise:</p> <pre><code>giddy init          # Shows a prompt to help you create a config\n</code></pre> <p>Or create a minimal config:</p> <pre><code>paths:\n  - path: .\n    description: My project\n</code></pre>"},{"location":"troubleshooting/#server-wont-start","title":"Server won't start","text":"<p>Port already in use: <pre><code>giddy down            # Stop any existing server\ngiddy up --port 9000  # Use a different port\n</code></pre></p> <p>Check logs: <pre><code>giddy log             # Stream server logs\n</code></pre></p>"},{"location":"troubleshooting/#slow-indexing","title":"Slow indexing","text":"<p>First index takes time (~45s for 750 files on M1 Pro). Subsequent startups are faster because: - Unchanged files use cached embeddings - Only modified files get re-indexed</p> <p>If re-indexing is slow: 1. Check <code>giddy status</code> for progress 2. Consider excluding large directories via <code>ignore_patterns</code> in config 3. Check if files are being repeatedly modified</p>"},{"location":"troubleshooting/#high-memory-usage","title":"High memory usage","text":"<p>The embedding model uses ~500MB RAM. If this is a problem: - Use a smaller model: add <code>local_model: \"paraphrase-MiniLM-L3-v2\"</code> to config - Stop the server when not in use: <code>giddy down</code></p>"},{"location":"troubleshooting/#search-returns-irrelevant-results","title":"Search returns irrelevant results","text":"<ol> <li>Add descriptions: Better path descriptions improve ranking</li> <li>Be more specific: Longer, more descriptive queries help</li> <li>Check file types: Only supported languages are indexed</li> </ol>"},{"location":"troubleshooting/#emacsvscode-not-connecting","title":"Emacs/VSCode not connecting","text":"<ol> <li>Ensure the server is running: <code>giddy status</code></li> <li>Check you're in a git repo or a directory with <code>.giddyanne.yaml</code></li> <li>For MCP: verify <code>giddy mcp</code> works from command line</li> </ol>"},{"location":"troubleshooting/#internal-files","title":"Internal Files","text":"<p>Runtime files live in a storage directory. The location depends on mode:</p> Mode Storage directory Config mode (<code>.giddyanne.yaml</code> found) <code>.giddyanne/</code> in the project root Git-only mode (no config) <code>$TMPDIR/giddyanne/&lt;project&gt;-&lt;hash&gt;/</code> <pre><code>&lt;storage-dir&gt;/\n\u251c\u2500\u2500 all-MiniLM-L6-v2/     # Named after your embedding model\n\u2502   \u251c\u2500\u2500 vectors.lance/    # Vector database (LanceDB)\n\u2502   \u2514\u2500\u2500 mcp.log           # MCP server log\n\u251c\u2500\u2500 -8000.log             # HTTP server log (host-port.log)\n\u2514\u2500\u2500 -8000.pid             # Server PID file (host-port.pid)\n</code></pre> <p>In git-only mode, the tmp path is deterministic \u2014 the same project always gets the same path. Indexes rebuild if the temp directory gets cleaned.</p>"},{"location":"troubleshooting/#log-files","title":"Log Files","text":"<p>HTTP server log (<code>&lt;storage-dir&gt;/&lt;host&gt;-&lt;port&gt;.log</code>): <pre><code>giddy log                  # Stream live (finds the right location automatically)\n</code></pre></p> <p>Contains indexing progress, file watch events, and search requests.</p> <p>MCP server log (<code>&lt;storage-dir&gt;/&lt;model&gt;/mcp.log</code>):</p> <p>Contains MCP protocol messages and search requests from Claude Code.</p>"},{"location":"troubleshooting/#pid-files","title":"PID Files","text":"<p>The <code>.pid</code> file tracks the running server process. If <code>giddy status</code> says the server is running but it's not responding, the PID file may be stale:</p> <pre><code>giddy clean --force   # Remove all storage data\ngiddy up              # Restart fresh\n</code></pre>"},{"location":"troubleshooting/#database-files","title":"Database Files","text":"<p>The <code>vectors.lance/</code> directory is a LanceDB database containing your indexed embeddings.</p> <p>To force a re-index (keeps logs): <pre><code>giddy drop\n</code></pre></p> <p>To reset everything (removes all .giddyanne data): <pre><code>giddy clean        # prompts for confirmation\ngiddy clean --force  # no confirmation\n</code></pre></p>"},{"location":"blog/","title":"Blog","text":""}]}